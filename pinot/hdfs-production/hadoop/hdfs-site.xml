<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>dfs.nameservices</name>
    <value>cluster0</value>
  </property>
  <property>
    <name>dfs.nameservice.id</name>
    <value>cluster0</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.cluster0</name>
    <value>rm-bd-hadoop-01.bigdata.digikala.com,rm-bd-hadoop-02.bigdata.digikala.com</value>
  </property>
  <property>
    <name>dfs.ha.namenode.id</name>
    <value>rm-bd-hadoop-01.bigdata.digikala.com</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.cluster0.rm-bd-hadoop-01.bigdata.digikala.com</name>
    <value>rm-bd-hadoop-01.bigdata.digikala.com:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-bind-host.cluster0.rm-bd-hadoop-01.bigdata.digikala.com</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.cluster0.rm-bd-hadoop-02.bigdata.digikala.com</name>
    <value>rm-bd-hadoop-02.bigdata.digikala.com:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-bind-host.cluster0.rm-bd-hadoop-02.bigdata.digikala.com</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.cluster0.rm-bd-hadoop-01.bigdata.digikala.com</name>
    <value>rm-bd-hadoop-01.bigdata.digikala.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-bind-host.cluster0.rm-bd-hadoop-01.bigdata.digikala.com</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.cluster0.rm-bd-hadoop-02.bigdata.digikala.com</name>
    <value>rm-bd-hadoop-02.bigdata.digikala.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-bind-host.cluster0.rm-bd-hadoop-02.bigdata.digikala.com</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://rm-bd-hadoop-03.bigdata.digikala.com:8485/cluster0</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/bigdata/hdfs/dfs/journaldir</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.cluster0</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
   <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/bigdata/hdfs/.ssh/id_rsa</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///bigdata/hdfs/dfs/data</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///bigdata/hdfs/dfs/name</value>
  </property>
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>hdfs</value>
  </property>
  <property>
    <name>fs.permissions.umask-mode</name>
    <value>002</value>
  </property>
  <property>
    <name>dfs.hosts.exclude</name>
    <value>/bigdata/hdfs/bin/hadoop/etc/hadoop/dfs.hosts.exclude</value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
    <final>true</final>
  </property>
  <property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.support.append</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.write.stale.datanode.ratio</name>
    <value>0.5f</value>
  </property>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>32</value>
  </property>
  <property>
    <name>dfs.namenode.service.handler.count</name>
    <value>16</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved</name>
    <value>1073741824</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>700</value>
  </property>
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>4096</value>
  </property>
  <property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
  </property>

  <property>
    <name>dfs.replication.max</name>
    <value>50</value>
  </property>
  <property>
    <name>dfs.namenode.replication.min</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>3600</value>
  </property>
  <property>
    <name>dfs.namenode.audit.log.async</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.num-threads</name>
    <value>10</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.timeout.millis</name>
    <value>1000</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/bigdata/hdfs/dn_sockets/dn._PORT</value>
  </property>
  <property>
    <name>dfs.namenode.acls.enabled</name>
    <value>true</value>
  </property>
</configuration>
